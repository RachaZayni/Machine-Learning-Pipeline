{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYLIPQM7sFJp"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade scikit-learn scikeras[tensorflow]\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder,LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "#!pip install scikeras[tensorflow]\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input,InputLayer\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# -------------------------------\n",
        "# 1. Load and Preprocess Dataset\n",
        "# -------------------------------\n",
        "social_df = pd.read_csv(\"social_media_vs_productivity.csv\")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Split Data FIRST to prevent leakage\n",
        "# -------------------------------\n",
        "X = social_df.drop(columns=['actual_productivity_score'])  # keep score aside temporarily\n",
        "y_score = social_df['actual_productivity_score']\n",
        "\n",
        "X_train_raw, X_test_raw, y_score_train, y_score_test = train_test_split(\n",
        "    X, y_score, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Reattach for easier processing\n",
        "train_df = X_train_raw.copy()\n",
        "train_df['actual_productivity_score'] = y_score_train\n",
        "\n",
        "test_df = X_test_raw.copy()\n",
        "test_df['actual_productivity_score'] = y_score_test\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Handle missing values (based on training data)\n",
        "# -------------------------------\n",
        "missing_cols = [\n",
        "    'daily_social_media_time', 'perceived_productivity_score', 'actual_productivity_score',\n",
        "    'stress_level', 'sleep_hours', 'screen_time_before_sleep', 'job_satisfaction_score'\n",
        "]\n",
        "\n",
        "# Fill train\n",
        "for col in missing_cols:\n",
        "    skewness = train_df[col].skew()\n",
        "    if abs(skewness) < 0.5:\n",
        "        train_df[col] = train_df[col].fillna(train_df[col].mean())\n",
        "    else:\n",
        "        train_df[col] = train_df[col].fillna(train_df[col].median())\n",
        "\n",
        "# Fill test using train stats\n",
        "for col in missing_cols:\n",
        "    skewness = train_df[col].skew()\n",
        "    if abs(skewness) < 0.5:\n",
        "        test_df[col] = test_df[col].fillna(train_df[col].mean())\n",
        "    else:\n",
        "        test_df[col] = test_df[col].fillna(train_df[col].median())\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Remove outliers from training set only\n",
        "# -------------------------------\n",
        "\n",
        "numeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "Q1 = train_df[numeric_cols].quantile(0.25)\n",
        "Q3 = train_df[numeric_cols].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "non_outliers_mask = ~((train_df[numeric_cols] < lower_bound) | (train_df[numeric_cols] > upper_bound)).any(axis=1)\n",
        "train_df_clean = train_df[non_outliers_mask].reset_index(drop=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Categorize productivity\n",
        "# -------------------------------\n",
        "train_df_clean['productivity_category'], bins = pd.qcut(\n",
        "    train_df_clean['actual_productivity_score'], q=3,\n",
        "    labels=['Low', 'Medium', 'High'], retbins=True\n",
        ")\n",
        "# Apply same bins to test set\n",
        "test_df['productivity_category'] = pd.cut(\n",
        "    test_df['actual_productivity_score'],\n",
        "    bins=bins,\n",
        "    labels=['Low', 'Medium', 'High'],\n",
        "    include_lowest=True\n",
        ")\n",
        "test_df_clean = test_df.dropna(subset=['productivity_category']).reset_index(drop=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Define features and target\n",
        "# -------------------------------\n",
        "X_train = train_df_clean.drop(columns=['actual_productivity_score', 'productivity_category'])\n",
        "y_train = train_df_clean['productivity_category']\n",
        "\n",
        "X_test = test_df_clean.drop(columns=['actual_productivity_score', 'productivity_category'])\n",
        "y_test = test_df_clean['productivity_category']\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Preprocessing pipeline\n",
        "# -------------------------------\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Fit-transform train / transform test\n",
        "# -------------------------------\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "# After fitting\n",
        "scaler = preprocessor.named_transformers_['num']\n",
        "\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Feature selection with SelectKBest\n",
        "# -------------------------------\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "selector.fit(X_train_processed, y_train_enc)\n",
        "\n",
        "X_train_selected = selector.transform(X_train_processed)\n",
        "X_test_selected = selector.transform(X_test_processed)\n",
        "\n",
        "\n",
        "num_features_names = numeric_features\n",
        "cat_features_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "all_feature_names = num_features_names + cat_features_names\n",
        "\n",
        "# Features sélectionnées\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "selected_feature_names = [all_feature_names[i] for i in selected_feature_indices]\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Build and Train Model\n",
        "# -------------------------------\n",
        "input_dim = X_train_selected.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "\n",
        "def create_model(num_hidden_layers=2, units=32, learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    for _ in range(num_hidden_layers):\n",
        "        model.add(Dense(units, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss=SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "keras_model = KerasClassifier(\n",
        "    model=create_model,\n",
        "    epochs=30,\n",
        "    batch_size=10,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'model__num_hidden_layers': [3, 4, 5],\n",
        "    'model__units': [32, 64],\n",
        "    'model__learning_rate': [0.01, 0.001],\n",
        "    'epochs': [20, 30],\n",
        "    'batch_size': [10, 20]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(keras_model, param_grid, cv=3)\n",
        "grid_result = grid.fit(X_train_selected, y_train)\n",
        "\n",
        "print(f\"Best score: {grid_result.best_score_} with params: {grid_result.best_params_}\")"
      ]
    }
  ]
}